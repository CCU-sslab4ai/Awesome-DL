# Awesome-DL
Collect useful articles, paper, implements

## Seq2seq
- [seq2seq系列相关实现与案例（feedback、peek、attention类型）](http://blog.csdn.net/sinat_26917383/article/details/75050225)
- [How to Visualize Your Recurrent Neural Network with Attention in Keras](https://medium.com/datalogue/attention-in-keras-1892773a4f22)
- [Improving Sequence Generation by GAN](https://youtu.be/Adi54-wp8Qk)
- [How to Use the TimeDistributed Layer for Long Short-Term Memory Networks in Python](https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/)

## Basic Concepts

- [Improving the way neural networks learn](http://neuralnetworksanddeeplearning.com/chap3.html)
- [设计embedding维数的时候有什么讲究？](https://www.zhihu.com/question/60648826)
- [深度机器学习中的batch的大小对学习效果有何影响？](https://www.zhihu.com/question/32673260)
- [HMM和RNN是什么关系？功效上两者有冲突重叠？](https://www.zhihu.com/question/57396443)
- [李宏毅ML Lecture 7: Backpropagation](https://youtu.be/ibJpTrp5mcE)
- [Negative log-likelihood function](http://www.cnblogs.com/ZJUT-jiangnan/p/5489047.html)

## Video
- [NIPS 2017](https://nips.cc/Conferences/2017/Schedule?type=Tutorial)
- [李宏毅教學總整理](http://speech.ee.ntu.edu.tw/~tlkagk/courses.html)
- [站在巨人的肩膀上, 迁移学习 Transfer Learning](https://youtu.be/fCEHdyLkjNE)
- [Stanford CS224N](https://www.youtube.com/watch?v=OQQ-W_63UgQ&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6)
- [NTU MiuLab](https://www.youtube.com/channel/UCyB2RBqKbxDPGCs1PokeUiA/videos)
- [12 effective approaches to attention based neural machine translation](https://youtu.be/XvOKXJxDn1U)

## Open Source
- [GANotebooks](https://github.com/tjwei/GANotebooks)
- [pytorch-book](https://github.com/chenyuntc/pytorch-book)
- [polyglot](http://polyglot.readthedocs.io/en/latest/)

## Transfer Learning
- [什么是迁移学习 (Transfer Learning)？这个领域历史发展前景如何？](https://www.zhihu.com/question/41979241)
- [Transfer Learning using Keras](https://towardsdatascience.com/transfer-learning-using-keras-d804b2e04ef8)

## GAN
- [BicycleGAN](https://dosudodl.wordpress.com/2018/02/08/nips-selected-paper-%E7%B9%BC%E6%9C%89%E5%90%8D%E7%9A%84cyclegan%E4%B9%8B%E5%BE%8C%E5%8F%88%E5%87%BA%E7%8F%BE%E7%9A%84bicyclegan/)

## Pre-processing
- [一文详解 Word2vec 之 Skip-Gram 模型（结构篇）](https://www.leiphone.com/news/201706/PamWKpfRFEI42McI.html)

## Keras
- [keras：3)Embedding层详解](http://blog.csdn.net/jiangpeng59/article/details/77533309)
- [keras：4)LSTM函数详解](http://blog.csdn.net/jiangpeng59/article/details/77646186)

## Embedding
- [Word Vector Representations: word2vec (Stanford NLP)](https://youtu.be/ERibwqs9p38)
- [GloVe (Stanford NLP)](https://youtu.be/ASn7ExxLZws)
